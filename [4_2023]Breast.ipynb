{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMGOy1BDAX2CHCJQB6+TpAS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TeddLe1804/TeddLe1804/blob/main/%08%5B4_2023%5DBreast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENVIRONMENT SETUP "
      ],
      "metadata": {
        "id": "ui6C1cnXAJal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADd yolox \n",
        "!git clone git@github.com:Megvii-BaseDetection/YOLOX.git\n",
        "cd YOLOX\n",
        "!pip3 install -v -e .  # or  python3 setup.py develop"
      ],
      "metadata": {
        "id": "Jou9_7tGI1bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDqJ1gVb9sUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de95623f-5c39-4f12-de06-38d828552901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing to /root/.config/pip/pip.conf\n",
            "Writing to /root/.config/pip/pip.conf\n"
          ]
        }
      ],
      "source": [
        "# https://stackoverflow.com/questions/46288847/how-to-suppress-pip-upgrade-warning\n",
        "!pip config set global.disable-pip-version-check true\n",
        "!pip config set global.root-user-action ignore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "ldWV5jRz_5k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n",
        "#This line of code is used to reduce the memory used of importing module if only the module is needed "
      ],
      "metadata": {
        "id": "lWSiLEoy-cUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /kaggle/tmp/libs"
      ],
      "metadata": {
        "id": "mCBSAcmKAhQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --index-url https://test.pypi.org/simple/ --no-deps torch2trt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOV4KiEkFomD",
        "outputId": "b3809c5a-7de8-4034-a5a4-36e249307348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://test.pypi.org/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch2trt\n",
            "  Downloading https://test-files.pythonhosted.org/packages/0c/c5/e568d6c4316a3fec84afefa67b603267e40bc997b4983f3fade84a8ab151/torch2trt-0.0.3-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch2trt\n",
            "Successfully installed torch2trt-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUo28vEfGGob",
        "outputId": "018a554c-086d-4ad3-a27b-e6c9f84090b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorrt\n",
            "  Downloading tensorrt-8.6.0-cp39-none-manylinux_2_17_x86_64.whl (819.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.2/819.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12\n",
            "  Downloading nvidia_cudnn_cu12-8.9.0.131-py3-none-manylinux1_x86_64.whl (719.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.6/719.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12\n",
            "  Downloading nvidia_cublas_cu12-12.1.0.26-py3-none-manylinux1_x86_64.whl (379.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.5/379.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.55-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.5/823.5 kB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu12, nvidia-cublas-cu12, nvidia-cudnn-cu12, tensorrt\n",
            "Successfully installed nvidia-cublas-cu12-12.1.0.26 nvidia-cuda-runtime-cu12-12.1.55 nvidia-cudnn-cu12-8.9.0.131 tensorrt-8.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch2trt"
      ],
      "metadata": {
        "id": "7Z85N6TfF6Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torch2trt import TRTModule\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "GBI262Ql-d_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_TORCH_VER = [int(x) for x in torch.__version__.split(\".\")[:2]]\n",
        "_TORCH11X = (_TORCH_VER >= [1, 10])"
      ],
      "metadata": {
        "id": "_PUqSmfTAVHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXTRACT ROI DETECTION "
      ],
      "metadata": {
        "id": "q73bEgZzPg5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def meshgrid(*tensors):\n",
        "    if _TORCH11X:\n",
        "        return torch.meshgrid(*tensors, indexing=\"ij\")\n",
        "    else:\n",
        "        return torch.meshgrid(*tensors)\n"
      ],
      "metadata": {
        "id": "qeBTNEaqG7t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_roi(img, kernel=(5,5)): \n",
        "  ori_h, ori_w = img.shape[:2]\n",
        "\n",
        "  # Clip percentage: implant, white lines \n",
        "  upper = np.percentile(img, 95)\n",
        "  img[img > upper] = np.min(img)\n",
        "\n",
        "  # Gaussian filtering to reduce noise \n",
        "  if kernel is not None: \n",
        "    img = cv2.GaussianBlur(img, kernel, 0)\n",
        "    _, img_bin = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "  #Dilation to improve the coutours connectivity using OpenCV Morphological Operations\n",
        "  element = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3), (-1, -1))\n",
        "  img_bin = cv2.dilate(img_bin, element)\n",
        "  contours, _ = cv2.findContours(img_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  if len(contours) == 0: \n",
        "    return None, None, None \n",
        "\n",
        "  areas = np.array([cv2.contourArea(contour) for contour in contours])\n",
        "  select_idx = np.argmax(areas) \n",
        "  contour = contours(select_idx)\n",
        "  area_pct = areas[select_idx] / (img.shape[0] * img.shape[1])\n",
        "  x0, y0, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "  #Min-max safety in roi \n",
        "  x1 = min(max(int(x0 + w), 0), ori_w)\n",
        "  y1 = min(max(int(y0 + h), 0), ori_h)\n",
        "  x0 = min(max(int(x0), 0), ori_w)\n",
        "  y0 = min(max(int(y0), 0), ori_h)\n",
        "  return [x0, y0, x1, y1], area_pct, None\n"
      ],
      "metadata": {
        "id": "Y46hcfQwHCfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoiExtractor: \n",
        "  def __init__(self,\n",
        "               engine_path, \n",
        "               input_size, \n",
        "               num_classes, \n",
        "               conf_thres = 0.5, \n",
        "               nms_thres = 0.9,\n",
        "               class_agnostic = False, \n",
        "               area_pct_thres = 0.04, \n",
        "               hw=None, \n",
        "               strides=None, \n",
        "               exp=None): \n",
        "               \n",
        "               self.input_size, self.input_h, self.input_w = input_size\n",
        "               self.num_classes = num_classes\n",
        "               self.conf_thres = conf_thres\n",
        "               self.nms_thres = nms_thres \n",
        "               self.class_agnostic = class_agnostic \n",
        "               self.area_pct_thres = area_pct_thres \n",
        "\n",
        "                  # Call Nvidia GPU \n",
        "               model = TRTModule()\n",
        "               model.load_state_dict(torch.load(engine_path))\n",
        "               self.model = model\n",
        "               if hw in None or strides is None: \n",
        "                 assert exp is not None \n",
        "                 self._set_meta(exp)\n",
        "               else: \n",
        "                self.hw = hw \n",
        "                self.strides = strides\n",
        "  def _set_meta(self, exp): \n",
        "    assert exp is not None\n",
        "    print(\"Start probing model metadata\")\n",
        "\n",
        "    torch_model = exp.get_model().cuda().eval()\n",
        "    _dummy = torch.ones(1,3, exp.test_size[0], exp.test_size[1])\n",
        "    torch_model(_dummy)\n",
        "    \n",
        "    #Set attribute\n",
        "    self.hw = torch_model.head.hw\n",
        "    self.strides = torch_model.head.strides \n",
        "\n",
        "    #Clean up \n",
        "    del torch_model, _dummy\n",
        "    import gc \n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print('Done probbing model metadata')\n",
        "\n",
        "\n",
        "    def decode_output(self, outputs):\n",
        "      dtype = outputs.type()\n",
        "      grids = []\n",
        "      strides = []\n",
        "      for (hszie, wsize), stride in zip(self.hw, self.strides): \n",
        "        yv, xv = meshgrid([torch.arange(hsize), torch.arange(wsize)])\n",
        "        grid = torch.stack((xv, yv), 2).view(1,-1,2)\n",
        "        grids.append(grid)\n",
        "        shape = grid.shape[:2]\n",
        "        strides.append(torch.full((*shape, 1), stride))\n",
        "      \n",
        "      grids = torch.cat(grids, dim=1).type(dtype)\n",
        "      strides = torch.cat(strides, dim=1).type(dtype)\n",
        "\n",
        "      output = torch.cat(\n",
        "          [(outputs[..., 0:2] + grids) * strides,\n",
        "           torch.exxp(outputs[..., 2:4]) * strides, outputs[..., 4:]],\n",
        "           dim = -1)\n",
        "      return outputs \n",
        "\n",
        "\n",
        "      def post_process(self,\n",
        "                       pred, \n",
        "                       conf_thres=0.5,\n",
        "                       nms_thres=0.9,\n",
        "                       class_agnostic=False):\n",
        "        '''\n",
        "        This code block takes in a tensor pred with dimensions [batch_size, num_boxes, num_coords] \n",
        "        and generates the coordinates of the corners of the bounding boxes represented by the pred tensor.\n",
        "\n",
        "First, a new tensor box_corner is created of the same shape as pred. \n",
        "The first dimension of box_corner (i.e. batch_size) is the same as that of pred. \n",
        "The second dimension of box_corner (i.e. num_boxes) is also the same as that of pred. \n",
        "The third dimension of box_corner (i.e. num_coords) is set to 4, \n",
        "as this tensor stores the coordinates of the top left and bottom right corners of the bounding boxes.\n",
        "\n",
        "The four coordinates of a bounding box are calculated as follows:\n",
        "\n",
        "box_corner[:, :, 0] = pred[:, :, 0] - pred[:, :, 2] / 2\n",
        "\n",
        "This computes the x-coordinate of the top left corner of each bounding box, \n",
        "which is equal to the x-coordinate of the center of the box (pred[:, :, 0])\n",
        " minus half of the width of the box (pred[:, :, 2] / 2).\n",
        "\n",
        "box_corner[:, :, 1] = pred[:, :, 1] - pred[:, :, 3] / 2\n",
        "\n",
        "This computes the y-coordinate of the top left corner of each bounding box, \n",
        "which is equal to the y-coordinate of the center of the box (pred[:, :, 1]) \n",
        "minus half of the height of the box (pred[:, :, 3] / 2).\n",
        "\n",
        "box_corner[:, :, 2] = pred[:, :, 0] + pred[:, :, 2] / 2\n",
        "\n",
        "This computes the x-coordinate of the bottom right corner of each bounding box,\n",
        " which is equal to the x-coordinate of the center of the box (pred[:, :, 0]) plus half of the width of the box (pred[:, :, 2] / 2).\n",
        "\n",
        "box_corner[:, :, 3] = pred[:, :, 1] + pred[:, :, 3] / 2\n",
        "\n",
        "This computes the y-coordinate of the bottom right corner of each bounding box, which is equal to the y-coordinate of the center of the box (pred[:, :, 1]) plus half of the height of the box (pred[:, :, 3] / 2).\n",
        "\n",
        "Finally, the first four coordinates of the pred tensor are updated with the corresponding coordinates from box_corner tensor. This effectively updates the coordinates of the bounding boxes from center coordinates to corner coordinates.\n",
        "        '''\n",
        "        box_corner = pred.new(pred.shape)\n",
        "        box_corner[:, :, 0] = pred[:, :, 0] - pred[:, :, 2] / 2 \n",
        "        box_corner[:, :, 1] = pred[:, :, 1] - pred[:, :, 3] / 2\n",
        "        box_corner[:, :, 2] = pred[:, :, 0] + pred[:, :, 2] / 2 \n",
        "        box_corner[:, :, 3] = pred[:, :, 1] + pred[:, :, 3] / 2 \n",
        "        pred[:, :, :4] = box_corner[:, :, :4]\n",
        "\n",
        "        output = [None for _ in range(len(pred))]\n",
        "        for i, image_pred in enumerate(pred): \n",
        "\n",
        "          # If none are remaining => process next image \n",
        "          if not image_pred.size(0): \n",
        "            continue \n",
        "\n",
        "          # Get score and class with highest confidence \n",
        "          class_conf, class_pred = torch.max(image_pred[:, 5:5 + self.num_classes], 1, keepdim=True) \n",
        "\n",
        "          #Conf mask \n",
        "          conf_mask = (image_pred[:, 4] * class_conf.squeeze() >= conf_thres).squeeze()\n",
        "\n",
        "          #Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
        "          detections = torch.cat(\n",
        "              (image_pred[:, :5], class_conf, class_pred.float()), 1\n",
        "          )\n",
        "\n",
        "          detections = detections[conf_mask]\n",
        "          \n",
        "          if not detections.size(0): \n",
        "            continue \n",
        "          \n",
        "          if class_agnostic: \n",
        "            nms_out_index = torchvision.ops.nms(\n",
        "                detections[:, :4],\n",
        "                detections[:, 4] * detections[:, 5], \n",
        "                nms_thres,\n",
        "            )\n",
        "          else:\n",
        "            nms_out_index = torchvision.ops.batch_nms(\n",
        "                detections[:, :4],\n",
        "                detections[:, 4] * detections[:, 5], \n",
        "                detections[:, 6], \n",
        "                nms_thres,\n",
        "            )\n",
        "\n",
        "            detections = detections[nms_out_index]\n",
        "            if output[i] is None: \n",
        "              output[i] = detections \n",
        "            else:\n",
        "              output[i] = torch.cat((output[i], detections))\n",
        "        return output \n",
        "\n",
        "\n",
        "        # Preprocess the single one \n",
        "        def preprocess_single(self, img: torch.Tensor): \n",
        "          ori_h = img.size[0]\n",
        "          ori_w = img.size[1]\n",
        "\n",
        "          ratio = min(self.input_h / ori_h, self.input / ori_w)\n",
        "          '''\n",
        "          This ratio is necessary because we want to ensure that the input image does not exceed the \n",
        "          maximum  allowed size for the model, and to preserve the original aspect ratio of the image.\n",
        "          '''\n",
        "\n",
        "          resized_img = F.interpolate(img.view(1,1, ori_h, ori_w),\n",
        "                                      mode = 'bilinear',\n",
        "                                      scale_factor = ratio,\n",
        "                                      recompute_scale_factor=True)[0,0]\n",
        "          \n",
        "          # Padding \n",
        "          padded_img = torch.full((self.input_h, self.input_w),\n",
        "                                  114,\n",
        "                                  dtype = resized_img.dtype,\n",
        "                                  device ='cuda') #Sẽ thay đổi thông số này --> TÌm thông số tối ưu\n",
        "\n",
        "          padded_img[:resized_img.size(0), :resized_img.size(1)] = resized_img \n",
        "          padded_img = padded_img.unsqueeze(-1).expand(-1, -1, 3) \n",
        "          # The \"dim -1\" parameter specifies that this new dimension should be added at the end of the tensor. This function is useful when working with multi-dimensional arrays that require an additional dimension for processing or operations.\n",
        "          #In PyTorch, unsqueeze() is a commonly used function that allows you to increase the number of dimensions in a tensor. For instance, you can use unsqueeze() to convert a 1D tensor into a 2D tensor by adding a new dimension. This can be used in a wide range of applications in machine learning, \n",
        "          #such as image processing, natural language processing, and more.\n",
        "          #Overall, unsqueeze() with dim -1 is a powerful tool for manipulating and processing multi-modal data in Python-based machine learning frameworks like PyTorch.\n",
        "\n",
        "          padded_img = padded_img.permuate(2, 0, 1) #Đảo chiều tensor \n",
        "          # [0, 1 ,2] --> Permute(2,0,1) --> [2, 1, 0]\n",
        "          padded_img = padded_img.float()\n",
        "          return padded_img, resized_img, ratio, ori_h, ori_w \n",
        "        \n",
        "        def detect_single(self, img): \n",
        "          \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lvAPatpDPsaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1LZeXXJA_Q5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}